{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess.data\n",
    "import models\n",
    "import importlib\n",
    "import util\n",
    "importlib.reload(util)\n",
    "importlib.reload(preprocess.data)\n",
    "importlib.reload(models)\n",
    "from models import BoW,CNN_Text\n",
    "from optimizer import AdamW\n",
    "from preprocess.data import load_mldoc_dataset\n",
    "from preprocess.data import generate_features\n",
    "from preprocess.moses import MosesTokenizer\n",
    "from preprocess.japanese_tokenizer import JapaneseTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:55<00:00, 108.04it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "dataset_path = \"/home/smg/nishikawa/corpus\"\n",
    "data = load_mldoc_dataset(dataset_path,\"en\",dev_size=0.05,seed=1)\n",
    "tokenizer = MosesTokenizer(lang=\"en\")\n",
    "ret = generate_features(data, tokenizer, 3, 500)\n",
    "word_vocab = ret['word_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_path = \"/home/smg/nishikawa/adversarial-examples-experiments/data/en.50k.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = KeyedVectors.load_word2vec_format(vec_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16308\n"
     ]
    }
   ],
   "source": [
    "dim_size = embedding.syn0.shape[1]\n",
    "cnt = 0\n",
    "word_embedding = np.random.uniform(low=-0.05, high=0.05, size=(len(word_vocab), dim_size))\n",
    "word_embedding[0] = np.zeros(dim_size)\n",
    "for word,index in word_vocab.items():\n",
    "    try:\n",
    "        word_embedding[index] = embedding.wv[word]\n",
    "        cnt += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, fold):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            args = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            logits = model(**args)\n",
    "            predictions += torch.argmax(logits, 1).to('cpu').tolist()\n",
    "            labels += batch['label'].to('cpu').tolist()\n",
    "\n",
    "    test_acc = accuracy_score(labels, predictions)\n",
    "    print(f'accuracy ({fold}): {test_acc:.4f}')\n",
    "\n",
    "    test_f1 = f1_score(labels, predictions, average='macro')\n",
    "    print(f'f-measure ({fold}): {test_f1:.4f}')\n",
    "\n",
    "    return test_acc, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dropout_prob = 0.1\n",
    "weight_decay = 0.01\n",
    "warmup_epochs = 2\n",
    "learning_rate = 1e-4\n",
    "train_data_loader = DataLoader(ret['train'], shuffle=False, batch_size=batch_size)\n",
    "dev_data_loader = DataLoader(ret['dev'], shuffle=False, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(ret['test'], shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Text(\n",
       "  (word_embedding): Embedding(25625, 300)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (fc1): Linear(in_features=300, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_Text(word_embedding, len(data.label_names), dropout_prob)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "epoch: 0 loss: 1.21541703: 100%|██████████| 179/179 [00:04<00:00, 37.27it/s]\n",
      "epoch: 1 loss: 0.84180301:   3%|▎         | 6/179 [00:00<00:03, 55.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.7133\n",
      "f-measure (dev): 0.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.99095184: 100%|██████████| 179/179 [00:03<00:00, 55.71it/s]\n",
      "epoch: 2 loss: 0.68079937:   3%|▎         | 6/179 [00:00<00:03, 56.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.7667\n",
      "f-measure (dev): 0.7593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.68979961: 100%|██████████| 179/179 [00:03<00:00, 55.65it/s]\n",
      "epoch: 3 loss: 0.66905886:   3%|▎         | 6/179 [00:00<00:03, 56.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.7667\n",
      "f-measure (dev): 0.7597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.77640224: 100%|██████████| 179/179 [00:03<00:00, 55.46it/s]\n",
      "epoch: 4 loss: 0.69656801:   3%|▎         | 6/179 [00:00<00:03, 50.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.7933\n",
      "f-measure (dev): 0.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 0.54939169: 100%|██████████| 179/179 [00:03<00:00, 54.93it/s]\n",
      "epoch: 5 loss: 0.35411870:   3%|▎         | 6/179 [00:00<00:03, 57.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8133\n",
      "f-measure (dev): 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 0.49870333: 100%|██████████| 179/179 [00:03<00:00, 55.82it/s]\n",
      "epoch: 6 loss: 0.58611852:   3%|▎         | 6/179 [00:00<00:03, 55.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8200\n",
      "f-measure (dev): 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: 0.44704977: 100%|██████████| 179/179 [00:03<00:00, 55.59it/s]\n",
      "epoch: 7 loss: 0.29551795:   3%|▎         | 6/179 [00:00<00:03, 54.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8267\n",
      "f-measure (dev): 0.8215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: 0.37531382: 100%|██████████| 179/179 [00:03<00:00, 55.67it/s]\n",
      "epoch: 8 loss: 0.56706083:   3%|▎         | 6/179 [00:00<00:03, 56.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8467\n",
      "f-measure (dev): 0.8420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: 0.29922867: 100%|██████████| 179/179 [00:03<00:00, 55.92it/s]\n",
      "epoch: 9 loss: 0.28344434:   3%|▎         | 6/179 [00:00<00:03, 55.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8533\n",
      "f-measure (dev): 0.8496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: 0.33228973: 100%|██████████| 179/179 [00:03<00:00, 56.11it/s]\n",
      "epoch: 10 loss: 0.26413321:   3%|▎         | 6/179 [00:00<00:03, 56.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8600\n",
      "f-measure (dev): 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: 0.38053289: 100%|██████████| 179/179 [00:03<00:00, 55.63it/s]\n",
      "epoch: 11 loss: 0.50394732:   3%|▎         | 6/179 [00:00<00:03, 56.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8667\n",
      "f-measure (dev): 0.8616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11 loss: 0.35683325: 100%|██████████| 179/179 [00:03<00:00, 55.79it/s]\n",
      "epoch: 12 loss: 0.33636507:   3%|▎         | 6/179 [00:00<00:03, 56.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8600\n",
      "f-measure (dev): 0.8557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12 loss: 0.34001446: 100%|██████████| 179/179 [00:03<00:00, 55.58it/s]\n",
      "epoch: 13 loss: 0.50555849:   3%|▎         | 6/179 [00:00<00:03, 55.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8667\n",
      "f-measure (dev): 0.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: 0.32532525: 100%|██████████| 179/179 [00:03<00:00, 55.60it/s]\n",
      "epoch: 14 loss: 0.32801828:   3%|▎         | 6/179 [00:00<00:03, 56.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8600\n",
      "f-measure (dev): 0.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14 loss: 0.30092722: 100%|██████████| 179/179 [00:03<00:00, 56.00it/s]\n",
      "epoch: 15 loss: 0.34444413:   3%|▎         | 6/179 [00:00<00:03, 56.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8533\n",
      "f-measure (dev): 0.8477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15 loss: 0.37427598: 100%|██████████| 179/179 [00:03<00:00, 55.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8533\n",
      "f-measure (dev): 0.8477\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "patience = 5\n",
    "best_val_acc = 0.0\n",
    "best_weights = None\n",
    "num_epochs_without_improvement = 0\n",
    "while True:\n",
    "    with tqdm(train_data_loader) as pbar:\n",
    "        model.train()\n",
    "        for batch in pbar:\n",
    "            args = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            logits = model(**args)\n",
    "            loss = F.cross_entropy(logits, torch.tensor(batch['label']).to(device))\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            pbar.set_description(f'epoch: {epoch} loss: {loss.item():.8f}')\n",
    "\n",
    "    epoch += 1\n",
    "    val_acc = evaluate(model, dev_data_loader, device, 'dev')[0]\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_weights = {k: v.to('cpu').clone() for k, v in model.state_dict().items()}\n",
    "        num_epochs_without_improvement = 0\n",
    "    else:\n",
    "        num_epochs_without_improvement += 1\n",
    "\n",
    "    if num_epochs_without_improvement >= patience:\n",
    "        model.load_state_dict(best_weights)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make distillation train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 179/179 [00:01<00:00, 141.50it/s]\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "teacher_logits = []\n",
    "answer = []\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    with tqdm(train_data_loader) as pbar:\n",
    "        model.eval()\n",
    "        for batch in pbar:\n",
    "            args = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            logits = model(**args)\n",
    "            teacher_logits.append(logits)\n",
    "            answer.append(torch.tensor(batch['label']).to(device))\n",
    "        \n",
    "#         loss = F.cross_entropy(logits, torch.tensor(batch['label']).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Text(\n",
       "  (word_embedding): Embedding(25625, 300)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (fc1): Linear(in_features=300, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_student = CNN_Text(word_embedding, len(data.label_names), dropout_prob)\n",
    "optimizer = optim.Adam(model_student.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "model_student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)\n",
    "def nl(input, target): return -input[range(target.shape[0]), target].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_for_distillation(student_logit, teacher_logit, temp=1.0): \n",
    "    student = F.softmax(student_logit / temp, dim = -1)\n",
    "    teacher = F.softmax(teacher_logit / temp, dim = -1)\n",
    "#     return - (student.log() * teacher).mean()\n",
    "    return - (student.log() * teacher).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "epoch: 0 loss: 0.34387890: 100%|██████████| 179/179 [00:03<00:00, 54.29it/s]\n",
      "epoch: 1 loss: 0.34435105:   3%|▎         | 6/179 [00:00<00:03, 57.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8600\n",
      "f-measure (dev): 0.8563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.34412763: 100%|██████████| 179/179 [00:03<00:00, 54.84it/s]\n",
      "epoch: 2 loss: 0.34531435:   3%|▎         | 6/179 [00:00<00:03, 55.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8600\n",
      "f-measure (dev): 0.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.34441283: 100%|██████████| 179/179 [00:03<00:00, 54.71it/s]\n",
      "epoch: 3 loss: 0.34548065:   3%|▎         | 6/179 [00:00<00:03, 55.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8600\n",
      "f-measure (dev): 0.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.34467620: 100%|██████████| 179/179 [00:03<00:00, 54.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (dev): 0.8333\n",
      "f-measure (dev): 0.8289\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "patience = 3\n",
    "best_val_acc = 0.0\n",
    "best_weights = None\n",
    "num_epochs_without_improvement = 0\n",
    "alpha = 1\n",
    "temp = 10\n",
    "while True:\n",
    "    with tqdm(train_data_loader) as pbar:\n",
    "        model_student.train()\n",
    "        for i, batch in enumerate(pbar):\n",
    "#             print(torch.tensor(batch['label']).to(device))\n",
    "#             break\n",
    "            args = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            logits = model_student(**args)\n",
    "            \n",
    "#             pred = softmax(logits)\n",
    "#             loss=nl(pred, answer[i])\n",
    "\n",
    "            loss_soft = cross_entropy_for_distillation(logits, teacher_logits[i], temp)\n",
    "            loss_hard = F.cross_entropy(logits, torch.tensor(batch['label']).to(device))\n",
    "#             loss = loss_soft * alpha * (temp**2) + loss_hard * (1-alpha)\n",
    "            loss = loss_soft * alpha + loss_hard * (1-alpha)\n",
    "#             loss = loss_soft\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "#             print(model_student.fc1.weight)\n",
    "            model_student.zero_grad()\n",
    "            pbar.set_description(f'epoch: {epoch} loss: {loss.item():.8f}')\n",
    "\n",
    "    epoch += 1\n",
    "    val_acc = evaluate(model_student, dev_data_loader, device, 'dev')[0]\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_weights = {k: v.to('cpu').clone() for k, v in model_student.state_dict().items()}\n",
    "        num_epochs_without_improvement = 0\n",
    "    else:\n",
    "        num_epochs_without_improvement += 1\n",
    "\n",
    "    if num_epochs_without_improvement >= patience:\n",
    "        model_student.load_state_dict(best_weights)\n",
    "        break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (test): 0.8853\n",
      "f-measure (test): 0.8842\n",
      "0.8853333333333333\n",
      "accuracy (test): 0.8783\n",
      "f-measure (test): 0.8772\n",
      "0.8783333333333333\n"
     ]
    }
   ],
   "source": [
    "model.word_embedding = nn.Embedding(word_embedding.shape[0], word_embedding.shape[1], padding_idx=0)\n",
    "model.word_embedding.weight = nn.Parameter(torch.FloatTensor(word_embedding))\n",
    "model.to(device)\n",
    "model_student.word_embedding = nn.Embedding(word_embedding.shape[0], word_embedding.shape[1], padding_idx=0)\n",
    "model_student.word_embedding.weight = nn.Parameter(torch.FloatTensor(word_embedding))\n",
    "model_student.to(device)\n",
    "print(evaluate(model, test_data_loader, device, 'test')[0])\n",
    "print(evaluate(model_student, test_data_loader, device, 'test')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_words = torch.zeros(len(word_vocab)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "100%|██████████| 179/179 [00:02<00:00, 81.21it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "# model.word_embedding.weight.requires_grad=True\n",
    "with tqdm(train_data_loader) as pbar:\n",
    "    model.train()\n",
    "    for batch in pbar:\n",
    "        args = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        logits = model(**args)\n",
    "        loss = F.cross_entropy(logits, torch.tensor(batch['label']).to(device))\n",
    "        loss.backward()\n",
    "        grads = model.word_embedding.weight.grad\n",
    "        cont_words += torch.norm(grads,dim=-1)\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2375 [00:00<?, ?it/s]/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "100%|██████████| 2375/2375 [00:30<00:00, 76.74it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "# model.word_embedding.weight.requires_grad=True\n",
    "with tqdm(train_data_loader) as pbar:\n",
    "    model_student.train()\n",
    "    for batch in pbar:\n",
    "        args = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        logits = model_student(**args)\n",
    "        loss = F.cross_entropy(logits, torch.tensor(batch['label']).to(device))\n",
    "        loss.backward()\n",
    "        grads = model_student.word_embedding.weight.grad\n",
    "        cont_words += torch.norm(grads,dim=-1)\n",
    "        model_student.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 5.4424e-03, 1.0888e+00, 6.1385e-02, 2.7390e-01, 9.6517e-01,\n",
       "        3.5304e-01, 2.0702e-01, 4.2818e-03, 1.2990e+00, 9.6241e-02, 2.8942e-01,\n",
       "        1.7207e-02, 1.5194e+00, 5.2167e-02, 2.5741e+00, 3.8088e-01, 1.3754e-01,\n",
       "        2.3049e+00, 1.1720e+00, 1.7243e-01, 1.8760e-01, 1.8355e-01, 3.0804e-01,\n",
       "        1.5545e-01, 1.3924e+00, 4.9646e-02, 7.0652e-02, 2.3712e-01, 5.2886e-02,\n",
       "        8.3334e-01, 6.6389e-04, 6.3035e-01, 2.0506e-01, 8.2625e-03, 1.6211e-01,\n",
       "        4.2441e-02, 1.9975e+00, 2.6601e-01, 1.2476e-02, 6.9769e-02, 2.1281e-02,\n",
       "        8.1884e-02, 5.8429e-02, 2.5545e-03, 9.7494e-03, 3.1470e-04, 1.0824e-01,\n",
       "        1.1977e-03, 6.2149e-02, 6.0036e-02, 3.0235e-01, 0.0000e+00, 1.0331e-03,\n",
       "        1.5695e-02, 1.8593e-01, 1.7621e-03, 8.3865e-02, 6.1081e-02, 6.3769e-06,\n",
       "        5.4858e-01, 4.4668e-03, 5.9412e-06, 1.2524e-02, 8.1610e-01, 4.5557e-01,\n",
       "        3.4215e-03, 1.2397e-01, 9.2432e-01, 6.2447e-01, 2.1389e-02, 2.1474e-02,\n",
       "        1.2341e+00, 2.5277e-03, 4.4823e-01, 3.0525e-02, 1.6629e-02, 0.0000e+00,\n",
       "        8.1993e-02, 6.1070e-02, 1.5744e-01, 1.0716e-04, 1.5066e-02, 8.3410e-01,\n",
       "        5.5895e-04, 2.5733e-03, 3.7634e-02, 2.4435e-01, 1.9441e+00, 4.0180e-01,\n",
       "        1.5170e-03, 1.1747e-01, 6.0608e-03, 3.4573e-01, 1.0385e+00, 3.4847e-02,\n",
       "        1.4103e-02, 7.4960e-01, 2.2921e-01, 7.1869e-03], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 695,   15,   18,   37,   88, 1958,  162,   13,  684, 1034,   25,  257,\n",
       "        1097,    9,  339,   72,  195,   19,  166,  276, 1252,    2,  432,   94,\n",
       "        3505,  557,    5,  111,   68,  369,  164,  771,   83,   30,   64,  804,\n",
       "         724,  320, 2343, 3250,  481,   97,  602,  299,  675,  191, 1096,  681,\n",
       "         772,  488,   32,  372, 2426,   69, 1480, 1773,  286, 1304, 1311,  456,\n",
       "        2810,   60,  189,  228,  887, 1367, 5644,  762,  234,  403, 1269, 2463,\n",
       "          65,  498,  982,   74, 1518, 2675, 4015, 2906,  718,  694,  449, 1386,\n",
       "         319,   89,  412, 1292,  198,  136,   16, 1322,  265, 2216,    6,  470,\n",
       "         328,   93,  251,  117], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_inds[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_inds_to_rank = dict()\n",
    "for i,imp_ind in enumerate(imp_inds):\n",
    "    imp_inds_to_rank[imp_ind.item()] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab_r = {v:k for k,v in word_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'said'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocab_r[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_inds = torch.argsort(cont_words, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab_r = {v:k for k,v in word_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_inds = set([id.item() for id in imp_inds[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/smg/nishikawa/.pyenv/versions/miniconda3-4.7.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15326\n"
     ]
    }
   ],
   "source": [
    "dim_size = embedding.syn0.shape[1]\n",
    "cnt = 0\n",
    "word_embedding = np.random.uniform(low=-0.05, high=0.05, size=(len(word_vocab), dim_size))\n",
    "word_embedding[0] = np.zeros(dim_size)\n",
    "for word,index in word_vocab.items():\n",
    "    if index in imp_inds:\n",
    "        continue\n",
    "    try:\n",
    "        word_embedding[index] = embedding.wv[word]\n",
    "        cnt += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (test): 0.8853\n",
      "f-measure (test): 0.8842\n",
      "accuracy (test): 0.8630\n",
      "f-measure (test): 0.8612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.863"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embedding = nn.Embedding(word_embedding.shape[0], word_embedding.shape[1], padding_idx=0)\n",
    "model.word_embedding.weight = nn.Parameter(torch.FloatTensor(word_embedding))\n",
    "model.to(device)\n",
    "model_student.word_embedding = nn.Embedding(word_embedding.shape[0], word_embedding.shape[1], padding_idx=0)\n",
    "model_student.word_embedding.weight = nn.Parameter(torch.FloatTensor(word_embedding))\n",
    "model_student.to(device)\n",
    "evaluate(model, test_data_loader, device, 'test')[0]\n",
    "evaluate(model_student, test_data_loader, device, 'test')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/188 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22726,    18,  4086,  ...,     0,     0,     0],\n",
      "        [  574,  4543,     2,  ...,     0,     0,     0],\n",
      "        [   15,    28,  4015,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 4947,  7791,  8733,  ...,     0,     0,     0],\n",
      "        [ 1729,    65,  4083,  ...,     0,     0,     0],\n",
      "        [10594, 10595,  5807,  ...,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with tqdm(test_data_loader) as pbar:\n",
    "    for batch in pbar:\n",
    "        print(batch['word_ids'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22726,    18,  4086,   804,   282,    13,  2476,  2477,  2449,  2478,\n",
       "         7655,  4809,    18,     6,   501,    25,   725,   307,    18,    19,\n",
       "         2476,   853,    88,    68,    69,    15,  2241,   166,   504,   649,\n",
       "         1394,   198,  2476,   189,  2759,  9700,   198,  1462,  4086,   804,\n",
       "         2476,  4809,    18,  3831,   384,   113,   184,    18,    69,    15,\n",
       "          853,   117,    87,    65,  5455,   166,    69,  1324,   237,  5043,\n",
       "           69,    18,    60,  1387,    15,   853,   614,    89,   111,     2,\n",
       "         2556, 22726,   109,  1730,   166,  3517,    25,  3429,   725,   307,\n",
       "           18,   166,  6821,    15,  4809,  1288,     9,  3304, 13133,   166,\n",
       "           19,  2152,   905,  5506, 22726,     6,     7,  7888,   166,  2208,\n",
       "         3779,   730,    15,  3244,    13,    15, 13019,  9762,    18,  1789,\n",
       "         4086,   804,   282,    18,     6,  1413,     9, 22726,     2,  2975,\n",
       "           18,    15,   853,  1387,  1673,   807,    88, 22726,   109,   279,\n",
       "        12307,   166,  1014,    87,  8802,  2476,  4809,  5219,   977,   111,\n",
       "        13630,  3084,     9, 10607, 22726,   189,  2398,  2476,  4809,   282,\n",
       "         5494,  4292,   674, 10938,     9, 11178,    72,     9,   174,    87,\n",
       "         1705,  4086,   804,    72,  4809,   960,  5494,  2208,    72,    57,\n",
       "         2840,    18,  1446,    87,  2748,     9,    55,  1435,  3429,    87,\n",
       "          426,    25,   649,   167,  1215,   198,    15,   695,    11,  4086,\n",
       "          804,  4809,   282,     9,  1358,    68, 20733,   934,  4809,   771,\n",
       "         4154,   772,  1630,  1857,  2547,   772,  9160,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['word_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_ids': array([22726,    18,  4086,   804,   282,    13,  2476,  2477,  2449,\n",
       "         2478,  7655,  4809,    18,     6,   501,    25,   725,   307,\n",
       "           18,    19,  2476,   853,    88,    68,    69,    15,  2241,\n",
       "          166,   504,   649,  1394,   198,  2476,   189,  2759,  9700,\n",
       "          198,  1462,  4086,   804,  2476,  4809,    18,  3831,   384,\n",
       "          113,   184,    18,    69,    15,   853,   117,    87,    65,\n",
       "         5455,   166,    69,  1324,   237,  5043,    69,    18,    60,\n",
       "         1387,    15,   853,   614,    89,   111,     2,  2556, 22726,\n",
       "          109,  1730,   166,  3517,    25,  3429,   725,   307,    18,\n",
       "          166,  6821,    15,  4809,  1288,     9,  3304, 13133,   166,\n",
       "           19,  2152,   905,  5506, 22726,     6,     7,  7888,   166,\n",
       "         2208,  3779,   730,    15,  3244,    13,    15, 13019,  9762,\n",
       "           18,  1789,  4086,   804,   282,    18,     6,  1413,     9,\n",
       "        22726,     2,  2975,    18,    15,   853,  1387,  1673,   807,\n",
       "           88, 22726,   109,   279, 12307,   166,  1014,    87,  8802,\n",
       "         2476,  4809,  5219,   977,   111, 13630,  3084,     9, 10607,\n",
       "        22726,   189,  2398,  2476,  4809,   282,  5494,  4292,   674,\n",
       "        10938,     9, 11178,    72,     9,   174,    87,  1705,  4086,\n",
       "          804,    72,  4809,   960,  5494,  2208,    72,    57,  2840,\n",
       "           18,  1446,    87,  2748,     9,    55,  1435,  3429,    87,\n",
       "          426,    25,   649,   167,  1215,   198,    15,   695,    11,\n",
       "         4086,   804,  4809,   282,     9,  1358,    68, 20733,   934,\n",
       "         4809,   771,  4154,   772,  1630,  1857,  2547,   772,  9160,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]),\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 4031.21it/s]\n"
     ]
    }
   ],
   "source": [
    "adv_test = []\n",
    "for test_data in tqdm(ret['test']):\n",
    "    rank_list = [imp_inds_to_rank[input_id] for input_id in test_data['word_ids'][test_data['word_ids']!=0]]\n",
    "    index_list = np.argsort(rank_list)\n",
    "    adv = copy.deepcopy(test_data['word_ids'])\n",
    "#     print(int(len(index_list) * 0.1))\n",
    "    adv[index_list[:int(len(index_list) * 0.7)]] = 0\n",
    "    adv_data = dict()\n",
    "    adv_data['word_ids'] = adv\n",
    "    adv_data['label'] = test_data['label']\n",
    "    adv_test.append(adv_data)\n",
    "#     break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[149,\n",
       " 25,\n",
       " 123,\n",
       " 14,\n",
       " 120,\n",
       " 86,\n",
       " 61,\n",
       " 6,\n",
       " 5,\n",
       " 84,\n",
       " 46,\n",
       " 125,\n",
       " 49,\n",
       " 23,\n",
       " 108,\n",
       " 71,\n",
       " 88,\n",
       " 64,\n",
       " 66,\n",
       " 135,\n",
       " 81]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n"
     ]
    }
   ],
   "source": [
    "for inputid in adv[index_list[:int(len(index_list) * 0.1)]]:\n",
    "    print(word_vocab_r[inputid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-140-5c1038c6b640>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-140-5c1038c6b640>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a = \"Stan Wraight, vice president sales of KLM Royal Dutch Airlines NV Cargo, will leave on May 1, a KLM statement said. \"The decision to end his service with KLM was taken jointly with executive vice president KLM Cargo, Jacques Ancher some time ago,\" the statement said. He is leaving to \"pursue other interests\", it added. The statement added that at Ancher's request Wraight had agreed to stay on until May 1, to assist the cargo division in completing transformation to a full unit structure. Wraight will be returning to North America following the completion of the restructuring. Paul Elich, deputy vice president sales, will act in Wraight's place, the statement added. Industry sources said Wraight had been tipped to succeed Ancher. He joined KLM Cargo Handling department at Montreal Dorval Airport in 1965. Wraight was appointed KLM Cargo sales manager UK & Ireland in 1984 and in 1992 he became vice president and cargo area manager North and East Asia, Australia. He remained in this position until he took on his last role with the company as vice president cargo sales in 1995. -Reuters Air Cargo Newsroom Tel+44 171 542-7706 Fax+5017\"\u001b[0m\n\u001b[0m                                                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = \"Stan Wraight, vice president sales of KLM Royal Dutch Airlines NV Cargo, will leave on May 1, a KLM statement said. \"The decision to end his service with KLM was taken jointly with executive vice president KLM Cargo, Jacques Ancher some time ago,\" the statement said. He is leaving to \"pursue other interests\", it added. The statement added that at Ancher's request Wraight had agreed to stay on until May 1, to assist the cargo division in completing transformation to a full unit structure. Wraight will be returning to North America following the completion of the restructuring. Paul Elich, deputy vice president sales, will act in Wraight's place, the statement added. Industry sources said Wraight had been tipped to succeed Ancher. He joined KLM Cargo Handling department at Montreal Dorval Airport in 1965. Wraight was appointed KLM Cargo sales manager UK & Ireland in 1984 and in 1992 he became vice president and cargo area manager North and East Asia, Australia. He remained in this position until he took on his last role with the company as vice president cargo sales in 1995. -Reuters Air Cargo Newsroom Tel+44 171 542-7706 Fax+5017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_ids': array([  574,  4543,     2,   791,  1101,   872,  1886,  6086,  8997,\n",
       "          166,    25,   854,   620,   166, 14195,    19, 22706,   872,\n",
       "          166,  1014,     0,  1737,  1790,  1084,     0,  4548,  4549,\n",
       "          166,  1729,     9,   755,  1706,  4787,     0,  4548,   229,\n",
       "          166,  1729,   111,  4550,    25,   739,   205,    18,  1749,\n",
       "           19,  3335,    72,    19,   302,    13,  5264,  4178,  3242,\n",
       "           51,    88,     0,  3294,   795,  1778,   574,  4543,   418,\n",
       "            0,  4544,     6,     7,  4995,   234,    19, 12476,   196,\n",
       "            0, 14823,  8988,     9,   574,  4543,    51,  3271,     0,\n",
       "         1020,  1865,    11,    72,     0,   173,     0,  2066,    98,\n",
       "            7,  3330,   234,    19,  5231,   494,     0,    33,    51,\n",
       "            0,  4954,   128,     9,  4544,  9234,  6086,    23,    64,\n",
       "         6406,    89,    60,    98,  7626,   546,   574,  4543,    72,\n",
       "         1729,   464,     0,   296,   872,   189,   620,  1557,    19,\n",
       "           69, 10550,  7236,    69,   166,  3242,    68,    69,    60,\n",
       "           30,    98,     7,    32,   428,  2428,    19, 19956,  3385,\n",
       "          198, 10278,   240,  4992,  6759,   971,  1166,   969,   757,\n",
       "          166,   176,     0,  3385,   228,     0,  4411,  2298,  1468,\n",
       "          130,  1884,    60,     0,    69,    87,   117,     9,  1551,\n",
       "          418,  1551,    18,  1461,  2222, 22727,  6332,     0,  1790,\n",
       "         5248,  5249,     2,  1789,     0,    51,   279,     0,  9929,\n",
       "         7823,    97,     0,   196,  1133,  1776,   363,   198,     0,\n",
       "         1737,   825,    51,   279,  1531,    11,  1502,  1269, 15017,\n",
       "         1025, 14003,  5271,  5272,     0, 12307,    11,     0,    97,\n",
       "            0,  1461,  1462,  3814,    18,    51,  2917,   166,   115,\n",
       "           60,   782,  1234,    87,     6, 17857,     0,  1461,  1462,\n",
       "            6,     7,  1881,     9,   168,   234,     0,  1020,   494,\n",
       "           33,     6,     7,  2068,    42,   395,    68,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]),\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_loader = DataLoader(adv_test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (test): 0.5837\n",
      "f-measure (test): 0.5563\n",
      "accuracy (test): 0.5803\n",
      "f-measure (test): 0.5484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5803333333333334"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, adv_data_loader, device, 'test')[0]\n",
    "evaluate(model_student, adv_data_loader, device, 'test')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [imp_inds_to_rank[input_id] for input_id in test_data['word_ids'][test_data['word_ids']!=0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tanzania': 1,\n",
       " '&apos;s': 2,\n",
       " 'largest': 3,\n",
       " 'state-owned': 4,\n",
       " 'bank': 5,\n",
       " 'will': 6,\n",
       " 'be': 7,\n",
       " 'split': 8,\n",
       " 'in': 9,\n",
       " 'three': 10,\n",
       " 'as': 11,\n",
       " 'part': 12,\n",
       " 'of': 13,\n",
       " 'reforms': 14,\n",
       " 'the': 15,\n",
       " 'financial': 16,\n",
       " 'sector': 17,\n",
       " ',': 18,\n",
       " 'a': 19,\n",
       " 'senior': 20,\n",
       " 'central': 21,\n",
       " 'official': 22,\n",
       " 'told': 23,\n",
       " 'reuters': 24,\n",
       " 'on': 25,\n",
       " 'tuesday.': 26,\n",
       " 'reform': 27,\n",
       " 'national': 28,\n",
       " 'commerce': 29,\n",
       " '(': 30,\n",
       " 'nbc': 31,\n",
       " ')': 32,\n",
       " 'which': 33,\n",
       " 'controls': 34,\n",
       " 'over': 35,\n",
       " '70': 36,\n",
       " 'percent': 37,\n",
       " 'banking': 38,\n",
       " 'carried': 39,\n",
       " 'out': 40,\n",
       " 'within': 41,\n",
       " 'next': 42,\n",
       " 'six': 43,\n",
       " 'months.': 44,\n",
       " 'established': 45,\n",
       " '1967': 46,\n",
       " 'under': 47,\n",
       " 'sweeping': 48,\n",
       " 'socialist': 49,\n",
       " 'policies': 50,\n",
       " 'has': 51,\n",
       " '172': 52,\n",
       " 'branches': 53,\n",
       " 'across': 54,\n",
       " 'this': 55,\n",
       " 'impoverished': 56,\n",
       " 'east': 57,\n",
       " 'african': 58,\n",
       " 'nation.': 59,\n",
       " 'it': 60,\n",
       " 'operates': 61,\n",
       " 'semi-autonomous': 62,\n",
       " 'although': 63,\n",
       " 'government': 64,\n",
       " 'is': 65,\n",
       " 'sole': 66,\n",
       " 'shareholder': 67,\n",
       " '.': 68,\n",
       " '&quot;': 69,\n",
       " 'very': 70,\n",
       " 'big': 71,\n",
       " 'and': 72,\n",
       " 'here.': 73,\n",
       " 'its': 74,\n",
       " 'action': 75,\n",
       " 'much': 76,\n",
       " 'influences': 77,\n",
       " 'all': 78,\n",
       " 'commercial': 79,\n",
       " 'banks': 80,\n",
       " 'sometimes': 81,\n",
       " 'even': 82,\n",
       " 'monetary': 83,\n",
       " 'wrong': 84,\n",
       " 'direction': 85,\n",
       " 'reuters.': 86,\n",
       " 'he': 87,\n",
       " 'said': 88,\n",
       " 'that': 89,\n",
       " 'splitting': 90,\n",
       " 'into': 91,\n",
       " 'smaller': 92,\n",
       " '--': 93,\n",
       " 'trade': 94,\n",
       " 'regional': 95,\n",
       " 'rural': 96,\n",
       " 'for': 97,\n",
       " 'would': 98,\n",
       " 'improve': 99,\n",
       " 'overall': 100,\n",
       " 'standard': 101,\n",
       " 'competition': 102,\n",
       " 'more': 103,\n",
       " 'fair': 104,\n",
       " 'help': 105,\n",
       " 'reduce': 106,\n",
       " 'monopoly': 107,\n",
       " 'power': 108,\n",
       " 'had': 109,\n",
       " 'while': 110,\n",
       " 'at': 111,\n",
       " 'same': 112,\n",
       " 'time': 113,\n",
       " 'capitalise': 114,\n",
       " 'make': 115,\n",
       " 'profitable': 116,\n",
       " 'said.': 117,\n",
       " 'task': 118,\n",
       " 'force': 119,\n",
       " 'set': 120,\n",
       " 'up': 121,\n",
       " 'incurred': 122,\n",
       " 'huge': 123,\n",
       " 'losses': 124,\n",
       " 'long': 125,\n",
       " 'time.': 126,\n",
       " 'officials': 127,\n",
       " 'say': 128,\n",
       " 'enterprises': 129,\n",
       " 'are': 130,\n",
       " 'mainly': 131,\n",
       " 'responsible': 132,\n",
       " 'bad': 133,\n",
       " 'debts.': 134,\n",
       " 'international': 135,\n",
       " 'fund': 136,\n",
       " 'imf': 137,\n",
       " 'resident': 138,\n",
       " 'representative': 139,\n",
       " 'welcomed': 140,\n",
       " 'news': 141,\n",
       " 'describing': 142,\n",
       " 'major': 143,\n",
       " 'step': 144,\n",
       " 'forward': 145,\n",
       " 'create': 146,\n",
       " 'sound': 147,\n",
       " 'local': 148,\n",
       " 'country': 149,\n",
       " 'our': 150,\n",
       " 'view': 151,\n",
       " 'positive': 152,\n",
       " 'development': 153,\n",
       " 'policy': 154,\n",
       " 'commitment': 155,\n",
       " 'enhanced': 156,\n",
       " 'structual': 157,\n",
       " 'adjustment': 158,\n",
       " 'facility': 159,\n",
       " 'esaf': 160,\n",
       " 'approved': 161,\n",
       " '$': 162,\n",
       " '234': 163,\n",
       " 'million': 164,\n",
       " 'three-year': 165,\n",
       " 'to': 166,\n",
       " 'last': 167,\n",
       " 'november': 168,\n",
       " 'marking': 169,\n",
       " 'return': 170,\n",
       " 'support': 171,\n",
       " 'first': 172,\n",
       " 'since': 173,\n",
       " '1992': 174,\n",
       " 'we': 175,\n",
       " 'see': 176,\n",
       " 'what': 177,\n",
       " 'needed': 178,\n",
       " 'here': 179,\n",
       " 'competitive': 180,\n",
       " 'system.': 181,\n",
       " 'recently': 182,\n",
       " 'months': 183,\n",
       " 'ago': 184,\n",
       " '78': 185,\n",
       " 'total': 186,\n",
       " 'deposits': 187,\n",
       " 'so': 188,\n",
       " 'was': 189,\n",
       " 'really': 190,\n",
       " 'price': 191,\n",
       " 'there': 192,\n",
       " 'no': 193,\n",
       " 'freely': 194,\n",
       " 'market': 195,\n",
       " 'but': 196,\n",
       " 'now': 197,\n",
       " 'with': 198,\n",
       " 'divided': 199,\n",
       " 'each': 200,\n",
       " 'units': 201,\n",
       " 'most': 202,\n",
       " 'have': 203,\n",
       " '20': 204,\n",
       " '30': 205,\n",
       " 'whatever': 206,\n",
       " 'does': 207,\n",
       " 'cannot': 208,\n",
       " 'deciding': 209,\n",
       " 'factor': 210,\n",
       " 'another': 211,\n",
       " 'important': 212,\n",
       " 'coming': 213,\n",
       " 'phase': 214,\n",
       " 'need': 215,\n",
       " 'use': 216,\n",
       " 'budgetary': 217,\n",
       " 'resources': 218,\n",
       " 'bail': 219,\n",
       " 'happened': 220,\n",
       " 'several': 221,\n",
       " 'occasions': 222,\n",
       " 'past': 223,\n",
       " 'plus': 224,\n",
       " 'because': 225,\n",
       " 'remove': 226,\n",
       " 'burden': 227,\n",
       " 'from': 228,\n",
       " 'back': 229,\n",
       " 'reforming': 230,\n",
       " 'one': 231,\n",
       " 'conditions': 232,\n",
       " 'demanded': 233,\n",
       " 'by': 234,\n",
       " 'release': 235,\n",
       " 'funds': 236,\n",
       " 'other': 237,\n",
       " 'include': 238,\n",
       " 'demands': 239,\n",
       " 'control': 240,\n",
       " 'borrowing': 241,\n",
       " 'limit': 242,\n",
       " 'spending': 243,\n",
       " 'increase': 244,\n",
       " 'revenue': 245,\n",
       " 'collection.': 246,\n",
       " 'once': 247,\n",
       " 'could': 248,\n",
       " 'lead': 249,\n",
       " 'encourage': 250,\n",
       " 'foreign': 251,\n",
       " 'seek': 252,\n",
       " 'business': 253,\n",
       " 'vietnam': 254,\n",
       " 'offer': 255,\n",
       " '50': 256,\n",
       " 'billion': 257,\n",
       " 'dong': 258,\n",
       " 'worth': 259,\n",
       " 'six-month': 260,\n",
       " 'treasury': 261,\n",
       " 'bills': 262,\n",
       " 'an': 263,\n",
       " 'auction': 264,\n",
       " 'wednesday': 265,\n",
       " 'state': 266,\n",
       " 'sale': 267,\n",
       " 'paper': 268,\n",
       " '27': 269,\n",
       " 'issued': 270,\n",
       " '2.5': 271,\n",
       " 'notes': 272,\n",
       " 'domestic': 273,\n",
       " 'institutions': 274,\n",
       " 'annual': 275,\n",
       " 'rate': 276,\n",
       " '8.20': 277,\n",
       " 'percent.': 278,\n",
       " 'been': 279,\n",
       " '17': 280,\n",
       " 'such': 281,\n",
       " 'sales': 282,\n",
       " 'far': 283,\n",
       " 'year': 284,\n",
       " 'through': 285,\n",
       " 'stock': 286,\n",
       " 'issued.': 287,\n",
       " 'aiming': 288,\n",
       " 'hold': 289,\n",
       " 'roughly': 290,\n",
       " 'every': 291,\n",
       " 'two': 292,\n",
       " 'weeks': 293,\n",
       " 'lay': 294,\n",
       " 'ground': 295,\n",
       " 'future': 296,\n",
       " 'secondary': 297,\n",
       " 'market.': 298,\n",
       " 'rates': 299,\n",
       " 'fallen': 300,\n",
       " 'sharply': 301,\n",
       " 'half': 302,\n",
       " 'partly': 303,\n",
       " 'due': 304,\n",
       " 'surplus': 305,\n",
       " 'march': 306,\n",
       " '1': 307,\n",
       " 'sold': 308,\n",
       " '15.00': 309,\n",
       " 'however': 310,\n",
       " 'bottomed': 311,\n",
       " 'recent': 312,\n",
       " 'tighter': 313,\n",
       " 'liquidity': 314,\n",
       " 'bids': 315,\n",
       " 'come': 316,\n",
       " 'higher': 317,\n",
       " 'than': 318,\n",
       " 'ministry': 319,\n",
       " 'finance': 320,\n",
       " 'prepared': 321,\n",
       " 'result': 322,\n",
       " 'less': 323,\n",
       " 'allocated': 324,\n",
       " 'offered': 325,\n",
       " 'us': 326,\n",
       " '=': 327,\n",
       " 'u.s.': 328,\n",
       " 'treasuries': 329,\n",
       " 'opened': 330,\n",
       " 'unchanged': 331,\n",
       " 'slightly': 332,\n",
       " 'lower': 333,\n",
       " 'amid': 334,\n",
       " 'cautiousness': 335,\n",
       " 'profit-taking': 336,\n",
       " 'busy': 337,\n",
       " 'calendar': 338,\n",
       " 'economic': 339,\n",
       " 'data': 340,\n",
       " 'day': 341,\n",
       " 'before': 342,\n",
       " 'scheduled': 343,\n",
       " 'federal': 344,\n",
       " 'reserve': 345,\n",
       " 'meeting.': 346,\n",
       " 'releases': 347,\n",
       " 'those': 348,\n",
       " 'personal': 349,\n",
       " 'income': 350,\n",
       " 'consumption': 351,\n",
       " 'december.': 352,\n",
       " 'former': 353,\n",
       " 'expected': 354,\n",
       " '0.8': 355,\n",
       " 'latter': 356,\n",
       " 'forecast': 357,\n",
       " '0.5': 358,\n",
       " 'rose': 359,\n",
       " 'indicator': 360,\n",
       " 'draw': 361,\n",
       " 'attention': 362,\n",
       " 'association': 363,\n",
       " 'purchasing': 364,\n",
       " 'management': 365,\n",
       " 'napm': 366,\n",
       " 'index.': 367,\n",
       " 'january': 368,\n",
       " 'index': 369,\n",
       " '1000': 370,\n",
       " 'est': 371,\n",
       " '/': 372,\n",
       " '1500': 373,\n",
       " 'gmt': 374,\n",
       " 'versus': 375,\n",
       " '53.8': 376,\n",
       " 'reading': 377,\n",
       " 'above': 378,\n",
       " 'points': 379,\n",
       " 'expanding': 380,\n",
       " 'economy.': 381,\n",
       " 'below': 382,\n",
       " 'indicates': 383,\n",
       " 'some': 384,\n",
       " 'sub-indices': 385,\n",
       " 'closely': 386,\n",
       " 'watched': 387,\n",
       " 'they': 388,\n",
       " 'reflect': 389,\n",
       " 'activity': 390,\n",
       " 'nation': 391,\n",
       " 'manufacturing': 392,\n",
       " 'also': 393,\n",
       " 'information': 394,\n",
       " 'month': 395,\n",
       " 'just': 396,\n",
       " 'completed': 397,\n",
       " 'early': 398,\n",
       " 'dealings': 399,\n",
       " '6-1': 400,\n",
       " '30-year': 401,\n",
       " 'benchmark': 402,\n",
       " 'bond': 403,\n",
       " 'quoted': 404,\n",
       " 'down': 405,\n",
       " '2': 406,\n",
       " '32': 407,\n",
       " '32.': 408,\n",
       " 'yield': 409,\n",
       " 'stood': 410,\n",
       " '6.79': 411,\n",
       " 'friday': 412,\n",
       " 'close.': 413,\n",
       " 'gets': 414,\n",
       " 'contruction': 415,\n",
       " 'december': 416,\n",
       " '0.7': 417,\n",
       " 'after': 418,\n",
       " '1.9': 419,\n",
       " 'jump': 420,\n",
       " 'certainly': 421,\n",
       " 'josh': 422,\n",
       " 'stiles': 423,\n",
       " 'strategist': 424,\n",
       " 'i.d.e.a': 425,\n",
       " 'took': 426,\n",
       " 'surprise': 427,\n",
       " 'like': 428,\n",
       " 'stronger': 429,\n",
       " 'concerned': 430,\n",
       " 'about': 431,\n",
       " 'prices': 432,\n",
       " 'paid': 433,\n",
       " 'component': 434,\n",
       " 'delivery': 435,\n",
       " 'times': 436,\n",
       " 'greenspan': 437,\n",
       " 'interested': 438,\n",
       " 'numbers': 439,\n",
       " 'noted': 440,\n",
       " 'indices': 441,\n",
       " 'showed': 442,\n",
       " 'i.d.e.a.': 443,\n",
       " 'looking': 444,\n",
       " '50.0': 445,\n",
       " '51.5': 446,\n",
       " 'lot': 447,\n",
       " 'shorts': 448,\n",
       " 'were': 449,\n",
       " 'squeezed': 450,\n",
       " 'depend': 451,\n",
       " 'components': 452,\n",
       " 'many': 453,\n",
       " 'economists': 454,\n",
       " 'including': 455,\n",
       " 'chairman': 456,\n",
       " 'alan': 457,\n",
       " 'believe': 458,\n",
       " 'potential': 459,\n",
       " 'therefore': 460,\n",
       " 'possible': 461,\n",
       " 'inflationary': 462,\n",
       " 'pressures': 463,\n",
       " 'if': 464,\n",
       " 'continues': 465,\n",
       " 'rise': 466,\n",
       " 'pressure': 467,\n",
       " 'fed': 468,\n",
       " 'raise': 469,\n",
       " 'interest': 470,\n",
       " 'later': 471,\n",
       " 'spring': 472,\n",
       " 'significant': 473,\n",
       " 'ease': 474,\n",
       " 'try': 475,\n",
       " 'test': 476,\n",
       " '112': 477,\n",
       " 'futures': 478,\n",
       " '6.75': 479,\n",
       " 'then': 480,\n",
       " 'dealers': 481,\n",
       " 'lighten': 482,\n",
       " 'ahead': 483,\n",
       " 'refunding': 484,\n",
       " 'sell': 485,\n",
       " 'three-': 486,\n",
       " '10-': 487,\n",
       " 'securities': 488,\n",
       " 'week': 489,\n",
       " 'quarterly': 490,\n",
       " 'financing': 491,\n",
       " 'operation.': 492,\n",
       " 'open': 493,\n",
       " 'committee': 494,\n",
       " 'policy-making': 495,\n",
       " 'arm': 496,\n",
       " 'meet': 497,\n",
       " 'tuesday': 498,\n",
       " 'wednesday.': 499,\n",
       " 'expect': 500,\n",
       " 'leave': 501,\n",
       " 'unchanged.': 502,\n",
       " 'short': 503,\n",
       " 'end': 504,\n",
       " 'maturity': 505,\n",
       " 'range': 506,\n",
       " 'three-month': 507,\n",
       " 'bill': 508,\n",
       " '5.02': 509,\n",
       " 'basis': 510,\n",
       " 'year-bill': 511,\n",
       " '5.28': 512,\n",
       " 'two-': 513,\n",
       " 'five-year': 514,\n",
       " 'yielding': 515,\n",
       " '5.93': 516,\n",
       " 'respectively': 517,\n",
       " 'ten-year': 518,\n",
       " '3': 519,\n",
       " '6.51': 520,\n",
       " 'n.a.': 521,\n",
       " 'desk': 522,\n",
       " '212-859-1679': 523,\n",
       " 'edinburgh': 524,\n",
       " 'festival': 525,\n",
       " 'ended': 526,\n",
       " 'weekend': 527,\n",
       " 'organisers': 528,\n",
       " 'claiming': 529,\n",
       " 'record-breaking': 530,\n",
       " 'box': 531,\n",
       " 'office': 532,\n",
       " 'success': 533,\n",
       " '50th': 534,\n",
       " 'arts': 535,\n",
       " 'ticket': 536,\n",
       " 'passed': 537,\n",
       " 'pound': 538,\n",
       " '1.5': 539,\n",
       " 'mark': 540,\n",
       " 'despite': 541,\n",
       " 'cancellations': 542,\n",
       " 'offerings': 543,\n",
       " 'mixed': 544,\n",
       " 'reception': 545,\n",
       " 'both': 546,\n",
       " 'critics': 547,\n",
       " 'public': 548,\n",
       " 'rest.': 549,\n",
       " 'fringe': 550,\n",
       " 'world': 551,\n",
       " 'art': 552,\n",
       " 'tickets': 553,\n",
       " 'different': 554,\n",
       " 'shows': 555,\n",
       " '650': 556,\n",
       " 'companies': 557,\n",
       " 'four': 558,\n",
       " 'weeks.': 559,\n",
       " 'military': 560,\n",
       " 'tattoo': 561,\n",
       " 'castle': 562,\n",
       " 'success.': 563,\n",
       " 'available': 564,\n",
       " 'performances': 565,\n",
       " 'precision': 566,\n",
       " 'drill': 567,\n",
       " 'warriors': 568,\n",
       " 'bands': 569,\n",
       " 'pipes': 570,\n",
       " 'drums': 571,\n",
       " 'scotland': 572,\n",
       " 'canada': 573,\n",
       " 'hong': 574,\n",
       " 'kong.': 575,\n",
       " 'film': 576,\n",
       " 'kicked': 577,\n",
       " 'off': 578,\n",
       " 'gala': 579,\n",
       " 'premiere': 580,\n",
       " 'featuring': 581,\n",
       " 'patron': 582,\n",
       " 'sean': 583,\n",
       " 'notched': 584,\n",
       " 'record': 585,\n",
       " '110,000': 586,\n",
       " 'one-week': 587,\n",
       " 'jazz': 588,\n",
       " 'topped': 589,\n",
       " 'previous': 590,\n",
       " 'sales.': 591,\n",
       " 'lord': 592,\n",
       " 'mayor': 593,\n",
       " 'eric': 594,\n",
       " 'milligan': 595,\n",
       " 'festivals': 596,\n",
       " 'add': 597,\n",
       " '140': 598,\n",
       " 'pounds': 599,\n",
       " '218': 600,\n",
       " 'city': 601,\n",
       " 'economy': 602,\n",
       " 'brian': 603,\n",
       " 'director': 604,\n",
       " 'i': 605,\n",
       " 'don': 606,\n",
       " '&apos;t': 607,\n",
       " 'think': 608,\n",
       " 'ever': 609,\n",
       " 'satisfying': 610,\n",
       " 'people': 611,\n",
       " 'street': 612,\n",
       " 'buy': 613,\n",
       " 'added': 614,\n",
       " 'strong': 615,\n",
       " 'wider': 616,\n",
       " 'audience': 617,\n",
       " 'suffered': 618,\n",
       " 'concerts': 619,\n",
       " 'not': 620,\n",
       " 'greatly': 621,\n",
       " 'affected': 622,\n",
       " 'withdrawal': 623,\n",
       " 'leading': 624,\n",
       " 'cancellation': 625,\n",
       " 'solo': 626,\n",
       " 'drama': 627,\n",
       " 'pieces': 628,\n",
       " 'robert': 629,\n",
       " 'elsinore': 630,\n",
       " 'equipment': 631,\n",
       " 'failure': 632,\n",
       " 'seven': 633,\n",
       " 'nicholas': 634,\n",
       " 'nicolas': 635,\n",
       " 'bartlett': 636,\n",
       " 'illness': 637,\n",
       " 'costly': 638,\n",
       " 'ines': 639,\n",
       " 'de': 640,\n",
       " 'castro': 641,\n",
       " 'opera': 642,\n",
       " 'contemporary': 643,\n",
       " 'scottish': 644,\n",
       " 'composer': 645,\n",
       " 'james': 646,\n",
       " 'judged': 647,\n",
       " 'fall': 648,\n",
       " 'his': 649,\n",
       " 'earlier': 650,\n",
       " 'disappointed': 651,\n",
       " 'houston': 652,\n",
       " 'grand': 653,\n",
       " 'saints': 654,\n",
       " 'acts': 655,\n",
       " 'thomson': 656,\n",
       " 'produced': 657,\n",
       " 'dancers': 658,\n",
       " 'american': 659,\n",
       " 'morris': 660,\n",
       " 'germany': 661,\n",
       " 'centre': 662,\n",
       " 'stage': 663,\n",
       " 'real': 664,\n",
       " 'characters': 665,\n",
       " 'sing': 666,\n",
       " 'wings': 667,\n",
       " 'disgruntled': 668,\n",
       " 'visitor': 669,\n",
       " 'left': 670,\n",
       " 'theatre': 671,\n",
       " 'robertson': 672,\n",
       " 'stephens': 673,\n",
       " '&amp;': 674,\n",
       " 'co': 675,\n",
       " 'analyst': 676,\n",
       " 'dan': 677,\n",
       " 'klesken': 678,\n",
       " 'raised': 679,\n",
       " 'rating': 680,\n",
       " 'shares': 681,\n",
       " 'storage': 682,\n",
       " 'devices': 683,\n",
       " 'inc': 684,\n",
       " 'term': 685,\n",
       " 'attractive': 686,\n",
       " 'performer': 687,\n",
       " 'based': 688,\n",
       " 'design': 689,\n",
       " 'wins': 690,\n",
       " 'outlook': 691,\n",
       " 'profitability': 692,\n",
       " '1997': 693,\n",
       " 'thursday': 694,\n",
       " 'company': 695,\n",
       " 'reported': 696,\n",
       " 'q4': 697,\n",
       " 'loss': 698,\n",
       " '0.15': 699,\n",
       " 'share': 700,\n",
       " 'revenues': 701,\n",
       " '9.7': 702,\n",
       " 'profit': 703,\n",
       " '12.6': 704,\n",
       " 'called': 705,\n",
       " 'earnings': 706,\n",
       " 'report': 707,\n",
       " 'better': 708,\n",
       " '1.19': 709,\n",
       " 'book-to-bill': 710,\n",
       " 'ratio': 711,\n",
       " '10': 712,\n",
       " 'new': 713,\n",
       " 'eight': 714,\n",
       " 'wireless': 715,\n",
       " 'communications': 716,\n",
       " 'argentina': 717,\n",
       " 'monday': 718,\n",
       " 'hopes': 719,\n",
       " 'collect': 720,\n",
       " 'between': 721,\n",
       " '4.3': 722,\n",
       " '4.4': 723,\n",
       " 'tax': 724,\n",
       " 'may': 725,\n",
       " 'compared': 726,\n",
       " '3.6': 727,\n",
       " '1996.': 728,\n",
       " 'expects': 729,\n",
       " 'following': 730,\n",
       " '754': 731,\n",
       " 'shortfall': 732,\n",
       " '1996': 733,\n",
       " '410': 734,\n",
       " 'deficit': 735,\n",
       " 'april': 736,\n",
       " 'year.': 737,\n",
       " 'registered': 738,\n",
       " 'june': 739,\n",
       " 'under-secretary': 740,\n",
       " 'guillermo': 741,\n",
       " 'rodriguez': 742,\n",
       " 'reporters': 743,\n",
       " 'collection': 744,\n",
       " 'figures': 745,\n",
       " 'look': 746,\n",
       " 'comparative': 747,\n",
       " 'taxes.': 748,\n",
       " 'taxes': 749,\n",
       " 'collected': 750,\n",
       " 'june.': 751,\n",
       " 'value': 752,\n",
       " 'around': 753,\n",
       " '1.6': 754,\n",
       " '1997.': 755,\n",
       " 'vat': 756,\n",
       " 'able': 757,\n",
       " 'keep': 758,\n",
       " 'level': 759,\n",
       " 'mostly': 760,\n",
       " 'imports.': 761,\n",
       " 'growth': 762,\n",
       " 'reiterated': 763,\n",
       " 'estimates': 764,\n",
       " '8.2': 765,\n",
       " 'whole': 766,\n",
       " '--david': 767,\n",
       " 'haskel': 768,\n",
       " 'buenos': 769,\n",
       " 'aires': 770,\n",
       " 'newsroom': 771,\n",
       " '+': 772,\n",
       " '541': 773,\n",
       " '318-0652': 774,\n",
       " 'hoping': 775,\n",
       " 'recess': 776,\n",
       " 'lawmakers': 777,\n",
       " 'tried': 778,\n",
       " 'together': 779,\n",
       " 'compromise': 780,\n",
       " 'plan': 781,\n",
       " 'clear': 782,\n",
       " 'senate': 783,\n",
       " 'expand': 784,\n",
       " 'parks': 785,\n",
       " 'system': 786,\n",
       " 'negotiations': 787,\n",
       " 'delicate': 788,\n",
       " 'aide': 789,\n",
       " 'california': 790,\n",
       " 'democratic': 791,\n",
       " 'sen.': 792,\n",
       " 'barbara': 793,\n",
       " 'boxer': 794,\n",
       " 'who': 795,\n",
       " 'pushing': 796,\n",
       " 'pass': 797,\n",
       " 'house': 798,\n",
       " 'weekend.': 799,\n",
       " 'backing': 800,\n",
       " 'white': 801,\n",
       " 'ready': 802,\n",
       " 'send': 803,\n",
       " 'president': 804,\n",
       " 'clinton': 805,\n",
       " 'congressional': 806,\n",
       " 'sources': 807,\n",
       " 'energy': 808,\n",
       " 'natural': 809,\n",
       " 'frank': 810,\n",
       " 'sponsored': 811,\n",
       " 'holding': 812,\n",
       " 'provisions': 813,\n",
       " 'continue': 814,\n",
       " 'contract': 815,\n",
       " 'held': 816,\n",
       " 'timber': 817,\n",
       " 'exclusive': 818,\n",
       " 'access': 819,\n",
       " 'alaska': 820,\n",
       " 'forest.': 821,\n",
       " 'republican': 822,\n",
       " 'pushed': 823,\n",
       " 'language': 824,\n",
       " 'administration': 825,\n",
       " 'dropped': 826,\n",
       " 'logging': 827,\n",
       " 'version': 828,\n",
       " 'deleted': 829,\n",
       " 'almost': 830,\n",
       " 'measures': 831,\n",
       " 'generated': 832,\n",
       " 'veto': 833,\n",
       " 'threat': 834,\n",
       " 'includes': 835,\n",
       " 'my': 836,\n",
       " 'top': 837,\n",
       " 'priorities': 838,\n",
       " 'legislation.': 839,\n",
       " 'san': 840,\n",
       " 'francisco': 841,\n",
       " 'acquire': 842,\n",
       " 'sterling': 843,\n",
       " 'forest': 844,\n",
       " 'york': 845,\n",
       " 'jersey': 846,\n",
       " 'highlands': 847,\n",
       " 'region': 848,\n",
       " 'establish': 849,\n",
       " 'prairie': 850,\n",
       " 'preserve': 851,\n",
       " 'kansas': 852,\n",
       " 'statement': 853,\n",
       " 'sunday': 854,\n",
       " 'vote.': 855,\n",
       " 'controversial': 856,\n",
       " 'opposed': 857,\n",
       " 'environmental': 858,\n",
       " 'groups': 859,\n",
       " 'allowed': 860,\n",
       " 'corporations': 861,\n",
       " 'become': 862,\n",
       " 'licensed': 863,\n",
       " 'sponsors': 864,\n",
       " 'money': 865,\n",
       " 'tackle': 866,\n",
       " '4': 867,\n",
       " 'backlog': 868,\n",
       " 'facilities': 869,\n",
       " 'repairs.': 870,\n",
       " 'majority': 871,\n",
       " 'leader': 872,\n",
       " 'trent': 873,\n",
       " 'lott': 874,\n",
       " 'mississippi': 875,\n",
       " 'brought': 876,\n",
       " 'proposal': 877,\n",
       " 'reinstate': 878,\n",
       " 'projects': 879,\n",
       " 'sure': 880,\n",
       " 'democrats': 881,\n",
       " 'or': 882,\n",
       " 'agree': 883,\n",
       " 'portuguese': 884,\n",
       " 'wood': 885,\n",
       " 'products': 886,\n",
       " 'group': 887,\n",
       " 'sonae': 888,\n",
       " 'industria': 889,\n",
       " 'capital': 890,\n",
       " 'investments': 891,\n",
       " 'brazil': 892,\n",
       " 'zimbabwe': 893,\n",
       " 'spokesman': 894,\n",
       " 'given': 895,\n",
       " 'involved': 896,\n",
       " 'necessary': 897,\n",
       " 'carlo': 898,\n",
       " 'did': 899,\n",
       " 'estimate': 900,\n",
       " 'size': 901,\n",
       " 'increase.': 902,\n",
       " '14': 903,\n",
       " 'escudos': 904,\n",
       " 'unit': 905,\n",
       " 'developing': 906,\n",
       " '135': 907,\n",
       " '33': 908,\n",
       " 'respectively.': 909,\n",
       " 'fell': 910,\n",
       " '61': 911,\n",
       " '1,500': 912,\n",
       " 'lisbon': 913,\n",
       " 'bourse': 914,\n",
       " 'volume': 915,\n",
       " '3,800': 916,\n",
       " 'police': 917,\n",
       " 'halted': 918,\n",
       " 'rail': 919,\n",
       " 'road': 920,\n",
       " 'traffic': 921,\n",
       " 'entering': 922,\n",
       " 'wellington': 923,\n",
       " 'parcel': 924,\n",
       " 'bomb': 925,\n",
       " 'inspector': 926,\n",
       " 'pat': 927,\n",
       " 'o': 928,\n",
       " '&apos;neill': 929,\n",
       " '7.00': 930,\n",
       " 'a.m.': 931,\n",
       " 'employee': 932,\n",
       " 'express': 933,\n",
       " 'air': 934,\n",
       " 'zealand': 935,\n",
       " 'couriers': 936,\n",
       " 'discovered': 937,\n",
       " 'package': 938,\n",
       " 'among': 939,\n",
       " 'mail': 940,\n",
       " 'ran': 941,\n",
       " 'put': 942,\n",
       " 'concrete': 943,\n",
       " 'pillars': 944,\n",
       " 'fairly': 945,\n",
       " 'game': 946,\n",
       " 'wasn': 947,\n",
       " '?': 948,\n",
       " 'courier': 949,\n",
       " 'area.': 950,\n",
       " 'quay': 951,\n",
       " 'commuter': 952,\n",
       " 'trains': 953,\n",
       " 'period': 954,\n",
       " '7': 955,\n",
       " '8.30': 956,\n",
       " 'a.m': 957,\n",
       " 'effect': 958,\n",
       " 'traffic.': 959,\n",
       " 'area': 960,\n",
       " 'main': 961,\n",
       " 'city.': 962,\n",
       " 'do': 963,\n",
       " 'obviously': 964,\n",
       " 'safety': 965,\n",
       " 'paramount': 966,\n",
       " 'lifted': 967,\n",
       " 'still': 968,\n",
       " 'being': 969,\n",
       " 'kept': 970,\n",
       " 'away': 971,\n",
       " 'removed': 972,\n",
       " 'scene': 973,\n",
       " 'again.': 974,\n",
       " 'explosives': 975,\n",
       " 'labour': 976,\n",
       " 'department': 977,\n",
       " 'examining': 978,\n",
       " 'take': 979,\n",
       " 'these': 980,\n",
       " 'things': 981,\n",
       " '-': 982,\n",
       " 'hear': 983,\n",
       " 'course': 984,\n",
       " 'happening': 985,\n",
       " 'know': 986,\n",
       " 'nothing': 987,\n",
       " 'else': 988,\n",
       " 'package.': 989,\n",
       " 'centimetres': 990,\n",
       " 'light': 991,\n",
       " 'students': 992,\n",
       " 'injured': 993,\n",
       " 'delivered': 994,\n",
       " 'malaysia': 995,\n",
       " 'collecting': 996,\n",
       " '64': 997,\n",
       " '4734746': 998,\n",
       " 'name': 999,\n",
       " 'gene': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [imp_inds_to_rank[input_id] for input_id in ret['test'][0]['word_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 21,\n",
       " 21,\n",
       " 27,\n",
       " 27]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list[:int(len(index_list) * 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = copy.deepcopy(ret['test'][0]['word_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv[index_list[:int(len(index_list) * 0.1)]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,   804,     0,    13,  2476,     0,  2449,\n",
       "        2478,     0,  4809,    18,     0,   501,     0,   725,     0,\n",
       "           0,    19,  2476,     0,    88,    68,    69,    15,  2241,\n",
       "           0,   504,   649,  1394,   198,  2476,   189,  2759,  9700,\n",
       "         198,  1462,  4086,   804,  2476,  4809,    18,  3831,   384,\n",
       "         113,   184,    18,    69,    15,   853,   117,    87,    65,\n",
       "        5455,   166,    69,  1324,   237,  5043,    69,    18,    60,\n",
       "        1387,    15,   853,   614,    89,   111,     2,  2556, 22726,\n",
       "         109,  1730,   166,  3517,    25,  3429,   725,   307,    18,\n",
       "         166,  6821,    15,  4809,  1288,     9,  3304, 13133,   166,\n",
       "          19,  2152,   905,  5506, 22726,     6,     7,  7888,   166,\n",
       "        2208,  3779,   730,    15,  3244,    13,    15, 13019,  9762,\n",
       "          18,  1789,  4086,   804,   282,    18,     6,  1413,     9,\n",
       "       22726,     2,  2975,    18,    15,   853,  1387,  1673,   807,\n",
       "          88, 22726,   109,   279, 12307,   166,  1014,    87,  8802,\n",
       "        2476,  4809,  5219,   977,   111, 13630,  3084,     9, 10607,\n",
       "       22726,   189,  2398,  2476,  4809,   282,  5494,  4292,   674,\n",
       "       10938,     9, 11178,    72,     9,   174,    87,  1705,  4086,\n",
       "         804,    72,  4809,   960,  5494,  2208,    72,    57,  2840,\n",
       "          18,  1446,    87,  2748,     9,    55,  1435,  3429,    87,\n",
       "         426,    25,   649,   167,  1215,   198,    15,   695,    11,\n",
       "        4086,   804,  4809,   282,     9,  1358,    68, 20733,   934,\n",
       "        4809,   771,  4154,   772,  1630,  1857,  2547,   772,  9160,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[[1,10]] = [4., 3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 4., 3., 1., 1., 1., 1., 1., 1., 1., 3., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{695: 0,\n",
       " 15: 1,\n",
       " 18: 2,\n",
       " 37: 3,\n",
       " 88: 4,\n",
       " 1958: 5,\n",
       " 162: 6,\n",
       " 13: 7,\n",
       " 684: 8,\n",
       " 1034: 9,\n",
       " 25: 10,\n",
       " 257: 11,\n",
       " 1097: 12,\n",
       " 9: 13,\n",
       " 339: 14,\n",
       " 72: 15,\n",
       " 195: 16,\n",
       " 19: 17,\n",
       " 166: 18,\n",
       " 276: 19,\n",
       " 1252: 20,\n",
       " 2: 21,\n",
       " 432: 22,\n",
       " 94: 23,\n",
       " 3505: 24,\n",
       " 557: 25,\n",
       " 5: 26,\n",
       " 111: 27,\n",
       " 68: 28,\n",
       " 369: 29,\n",
       " 164: 30,\n",
       " 771: 31,\n",
       " 83: 32,\n",
       " 30: 33,\n",
       " 64: 34,\n",
       " 804: 35,\n",
       " 724: 36,\n",
       " 320: 37,\n",
       " 2343: 38,\n",
       " 3250: 39,\n",
       " 481: 40,\n",
       " 97: 41,\n",
       " 602: 42,\n",
       " 299: 43,\n",
       " 675: 44,\n",
       " 191: 45,\n",
       " 1096: 46,\n",
       " 681: 47,\n",
       " 772: 48,\n",
       " 488: 49,\n",
       " 32: 50,\n",
       " 372: 51,\n",
       " 2426: 52,\n",
       " 69: 53,\n",
       " 1480: 54,\n",
       " 1773: 55,\n",
       " 286: 56,\n",
       " 1304: 57,\n",
       " 1311: 58,\n",
       " 456: 59,\n",
       " 2810: 60,\n",
       " 60: 61,\n",
       " 189: 62,\n",
       " 228: 63,\n",
       " 887: 64,\n",
       " 1367: 65,\n",
       " 5644: 66,\n",
       " 762: 67,\n",
       " 234: 68,\n",
       " 403: 69,\n",
       " 1269: 70,\n",
       " 2463: 71,\n",
       " 65: 72,\n",
       " 498: 73,\n",
       " 982: 74,\n",
       " 74: 75,\n",
       " 1518: 76,\n",
       " 2675: 77,\n",
       " 4015: 78,\n",
       " 2906: 79,\n",
       " 718: 80,\n",
       " 694: 81,\n",
       " 449: 82,\n",
       " 1386: 83,\n",
       " 319: 84,\n",
       " 89: 85,\n",
       " 412: 86,\n",
       " 1292: 87,\n",
       " 198: 88,\n",
       " 136: 89,\n",
       " 16: 90,\n",
       " 1322: 91,\n",
       " 265: 92,\n",
       " 2216: 93,\n",
       " 6: 94,\n",
       " 470: 95,\n",
       " 328: 96,\n",
       " 93: 97,\n",
       " 251: 98,\n",
       " 117: 99,\n",
       " 2592: 100,\n",
       " 1760: 101,\n",
       " 3672: 102,\n",
       " 700: 103,\n",
       " 135: 104,\n",
       " 261: 105,\n",
       " 121: 106,\n",
       " 379: 107,\n",
       " 359: 108,\n",
       " 282: 109,\n",
       " 418: 110,\n",
       " 1916: 111,\n",
       " 23: 112,\n",
       " 1461: 113,\n",
       " 51: 114,\n",
       " 263: 115,\n",
       " 713: 116,\n",
       " 344: 117,\n",
       " 4361: 118,\n",
       " 11: 119,\n",
       " 4087: 120,\n",
       " 696: 121,\n",
       " 478: 122,\n",
       " 5112: 123,\n",
       " 4: 124,\n",
       " 1799: 125,\n",
       " 674: 126,\n",
       " 1035: 127,\n",
       " 1101: 128,\n",
       " 1479: 129,\n",
       " 38: 130,\n",
       " 2171: 131,\n",
       " 4307: 132,\n",
       " 2676: 133,\n",
       " 2478: 134,\n",
       " 4115: 135,\n",
       " 109: 136,\n",
       " 604: 137,\n",
       " 130: 138,\n",
       " 87: 139,\n",
       " 1673: 140,\n",
       " 1023: 141,\n",
       " 1977: 142,\n",
       " 4626: 143,\n",
       " 141: 144,\n",
       " 894: 145,\n",
       " 2009: 146,\n",
       " 28: 147,\n",
       " 1462: 148,\n",
       " 405: 149,\n",
       " 284: 150,\n",
       " 886: 151,\n",
       " 98: 152,\n",
       " 203: 153,\n",
       " 1789: 154,\n",
       " 154: 155,\n",
       " 127: 156,\n",
       " 733: 157,\n",
       " 266: 158,\n",
       " 882: 159,\n",
       " 167: 160,\n",
       " 4424: 161,\n",
       " 1037: 162,\n",
       " 735: 163,\n",
       " 3523: 164,\n",
       " 2030: 165,\n",
       " 7: 166,\n",
       " 976: 167,\n",
       " 33: 168,\n",
       " 795: 169,\n",
       " 1426: 170,\n",
       " 494: 171,\n",
       " 137: 172,\n",
       " 253: 173,\n",
       " 1244: 174,\n",
       " 8000: 175,\n",
       " 977: 176,\n",
       " 2074: 177,\n",
       " 307: 178,\n",
       " 1112: 179,\n",
       " 5308: 180,\n",
       " 21: 181,\n",
       " 910: 182,\n",
       " 55: 183,\n",
       " 1172: 184,\n",
       " 2555: 185,\n",
       " 2222: 186,\n",
       " 522: 187,\n",
       " 196: 188,\n",
       " 7141: 189,\n",
       " 22: 190,\n",
       " 231: 191,\n",
       " 4109: 192,\n",
       " 267: 193,\n",
       " 1513: 194,\n",
       " 308: 195,\n",
       " 2093: 196,\n",
       " 2650: 197,\n",
       " 264: 198,\n",
       " 693: 199,\n",
       " 3326: 200,\n",
       " 2615: 201,\n",
       " 317: 202,\n",
       " 20: 203,\n",
       " 374: 204,\n",
       " 4255: 205,\n",
       " 1752: 206,\n",
       " 5449: 207,\n",
       " 890: 208,\n",
       " 341: 209,\n",
       " 345: 210,\n",
       " 1005: 211,\n",
       " 350: 212,\n",
       " 1817: 213,\n",
       " 245: 214,\n",
       " 363: 215,\n",
       " 35: 216,\n",
       " 244: 217,\n",
       " 489: 218,\n",
       " 1330: 219,\n",
       " 3248: 220,\n",
       " 431: 221,\n",
       " 80: 222,\n",
       " 865: 223,\n",
       " 24: 224,\n",
       " 2702: 225,\n",
       " 872: 226,\n",
       " 1757: 227,\n",
       " 4960: 228,\n",
       " 613: 229,\n",
       " 2842: 230,\n",
       " 5494: 231,\n",
       " 1281: 232,\n",
       " 103: 233,\n",
       " 2108: 234,\n",
       " 526: 235,\n",
       " 3504: 236,\n",
       " 5713: 237,\n",
       " 4086: 238,\n",
       " 292: 239,\n",
       " 1183: 240,\n",
       " 454: 241,\n",
       " 333: 242,\n",
       " 6537: 243,\n",
       " 1630: 244,\n",
       " 712: 245,\n",
       " 1399: 246,\n",
       " 1249: 247,\n",
       " 8820: 248,\n",
       " 388: 249,\n",
       " 2178: 250,\n",
       " 1489: 251,\n",
       " 4560: 252,\n",
       " 17: 253,\n",
       " 172: 254,\n",
       " 318: 255,\n",
       " 640: 256,\n",
       " 2193: 257,\n",
       " 393: 258,\n",
       " 2816: 259,\n",
       " 1737: 260,\n",
       " 4809: 261,\n",
       " 3587: 262,\n",
       " 2491: 263,\n",
       " 3795: 264,\n",
       " 611: 265,\n",
       " 4455: 266,\n",
       " 327: 267,\n",
       " 5658: 268,\n",
       " 306: 269,\n",
       " 2496: 270,\n",
       " 236: 271,\n",
       " 2520: 272,\n",
       " 354: 273,\n",
       " 8727: 274,\n",
       " 406: 275,\n",
       " 259: 276,\n",
       " 1076: 277,\n",
       " 409: 278,\n",
       " 1118: 279,\n",
       " 279: 280,\n",
       " 791: 281,\n",
       " 620: 282,\n",
       " 1113: 283,\n",
       " 273: 284,\n",
       " 1301: 285,\n",
       " 67: 286,\n",
       " 812: 287,\n",
       " 2528: 288,\n",
       " 1443: 289,\n",
       " 2088: 290,\n",
       " 340: 291,\n",
       " 2306: 292,\n",
       " 721: 293,\n",
       " 1819: 294,\n",
       " 278: 295,\n",
       " 2629: 296,\n",
       " 2627: 297,\n",
       " 1033: 298,\n",
       " 1790: 299,\n",
       " 91: 300,\n",
       " 4293: 301,\n",
       " 845: 302,\n",
       " 129: 303,\n",
       " 10408: 304,\n",
       " 1067: 305,\n",
       " 1562: 306,\n",
       " 1052: 307,\n",
       " 243: 308,\n",
       " 578: 309,\n",
       " 3644: 310,\n",
       " 237: 311,\n",
       " 186: 312,\n",
       " 1029: 313,\n",
       " 2797: 314,\n",
       " 2283: 315,\n",
       " 1905: 316,\n",
       " 485: 317,\n",
       " 1237: 318,\n",
       " 786: 319,\n",
       " 2598: 320,\n",
       " 4388: 321,\n",
       " 153: 322,\n",
       " 3708: 323,\n",
       " 1452: 324,\n",
       " 8357: 325,\n",
       " 736: 326,\n",
       " 107: 327,\n",
       " 2517: 328,\n",
       " 1519: 329,\n",
       " 3227: 330,\n",
       " 1529: 331,\n",
       " 4720: 332,\n",
       " 47: 333,\n",
       " 192: 334,\n",
       " 1889: 335,\n",
       " 2695: 336,\n",
       " 725: 337,\n",
       " 1615: 338,\n",
       " 4543: 339,\n",
       " 749: 340,\n",
       " 384: 341,\n",
       " 2423: 342,\n",
       " 275: 343,\n",
       " 4875: 344,\n",
       " 753: 345,\n",
       " 3116: 346,\n",
       " 4947: 347,\n",
       " 1333: 348,\n",
       " 353: 349,\n",
       " 2019: 350,\n",
       " 1729: 351,\n",
       " 3760: 352,\n",
       " 551: 353,\n",
       " 3649: 354,\n",
       " 1857: 355,\n",
       " 2071: 356,\n",
       " 1674: 357,\n",
       " 2350: 358,\n",
       " 1858: 359,\n",
       " 1856: 360,\n",
       " 2506: 361,\n",
       " 285: 362,\n",
       " 193: 363,\n",
       " 10: 364,\n",
       " 8971: 365,\n",
       " 1004: 366,\n",
       " 205: 367,\n",
       " 110: 368,\n",
       " 7897: 369,\n",
       " 1378: 370,\n",
       " 1011: 371,\n",
       " 739: 372,\n",
       " 4094: 373,\n",
       " 5743: 374,\n",
       " 12266: 375,\n",
       " 649: 376,\n",
       " 3847: 377,\n",
       " 342: 378,\n",
       " 1164: 379,\n",
       " 5600: 380,\n",
       " 1954: 381,\n",
       " 3273: 382,\n",
       " 706: 383,\n",
       " 532: 384,\n",
       " 3428: 385,\n",
       " 6533: 386,\n",
       " 466: 387,\n",
       " 808: 388,\n",
       " 8102: 389,\n",
       " 5674: 390,\n",
       " 5320: 391,\n",
       " 703: 392,\n",
       " 204: 393,\n",
       " 1834: 394,\n",
       " 15160: 395,\n",
       " 2664: 396,\n",
       " 2069: 397,\n",
       " 357: 398,\n",
       " 304: 399,\n",
       " 314: 400,\n",
       " 148: 401,\n",
       " 256: 402,\n",
       " 1203: 403,\n",
       " 510: 404,\n",
       " 248: 405,\n",
       " 2450: 406,\n",
       " 1421: 407,\n",
       " 2336: 408,\n",
       " 3449: 409,\n",
       " 3475: 410,\n",
       " 743: 411,\n",
       " 2284: 412,\n",
       " 601: 413,\n",
       " 1861: 414,\n",
       " 108: 415,\n",
       " 10571: 416,\n",
       " 57: 417,\n",
       " 752: 418,\n",
       " 183: 419,\n",
       " 676: 420,\n",
       " 1776: 421,\n",
       " 187: 422,\n",
       " 934: 423,\n",
       " 726: 424,\n",
       " 395: 425,\n",
       " 78: 426,\n",
       " 42: 427,\n",
       " 398: 428,\n",
       " 9350: 429,\n",
       " 1332: 430,\n",
       " 5383: 431,\n",
       " 4469: 432,\n",
       " 1815: 433,\n",
       " 1358: 434,\n",
       " 2449: 435,\n",
       " 1441: 436,\n",
       " 659: 437,\n",
       " 120: 438,\n",
       " 4123: 439,\n",
       " 3744: 440,\n",
       " 2372: 441,\n",
       " 1347: 442,\n",
       " 8026: 443,\n",
       " 574: 444,\n",
       " 3084: 445,\n",
       " 905: 446,\n",
       " 143: 447,\n",
       " 854: 448,\n",
       " 615: 449,\n",
       " 2812: 450,\n",
       " 2172: 451,\n",
       " 4047: 452,\n",
       " 3111: 453,\n",
       " 624: 454,\n",
       " 1929: 455,\n",
       " 915: 456,\n",
       " 2550: 457,\n",
       " 5322: 458,\n",
       " 3732: 459,\n",
       " 1025: 460,\n",
       " 1066: 461,\n",
       " 424: 462,\n",
       " 1083: 463,\n",
       " 6167: 464,\n",
       " 853: 465,\n",
       " 1084: 466,\n",
       " 614: 467,\n",
       " 9475: 468,\n",
       " 707: 469,\n",
       " 3249: 470,\n",
       " 1514: 471,\n",
       " 2260: 472,\n",
       " 2143: 473,\n",
       " 175: 474,\n",
       " 701: 475,\n",
       " 593: 476,\n",
       " 315: 477,\n",
       " 5089: 478,\n",
       " 5647: 479,\n",
       " 9781: 480,\n",
       " 7143: 481,\n",
       " 4082: 482,\n",
       " 8105: 483,\n",
       " 1296: 484,\n",
       " 202: 485,\n",
       " 540: 486,\n",
       " 3400: 487,\n",
       " 1327: 488,\n",
       " 745: 489,\n",
       " 1202: 490,\n",
       " 7769: 491,\n",
       " 1078: 492,\n",
       " 27: 493,\n",
       " 1523: 494,\n",
       " 2034: 495,\n",
       " 1310: 496,\n",
       " 40: 497,\n",
       " 271: 498,\n",
       " 2397: 499,\n",
       " 2839: 500,\n",
       " 1050: 501,\n",
       " 2113: 502,\n",
       " 2359: 503,\n",
       " 3399: 504,\n",
       " 8097: 505,\n",
       " 548: 506,\n",
       " 1060: 507,\n",
       " 2309: 508,\n",
       " 1566: 509,\n",
       " 491: 510,\n",
       " 2614: 511,\n",
       " 365: 512,\n",
       " 2347: 513,\n",
       " 3486: 514,\n",
       " 4451: 515,\n",
       " 816: 516,\n",
       " 407: 517,\n",
       " 7515: 518,\n",
       " 197: 519,\n",
       " 1373: 520,\n",
       " 3009: 521,\n",
       " 1626: 522,\n",
       " 11358: 523,\n",
       " 716: 524,\n",
       " 2174: 525,\n",
       " 3687: 526,\n",
       " 1291: 527,\n",
       " 5068: 528,\n",
       " 3901: 529,\n",
       " 268: 530,\n",
       " 2516: 531,\n",
       " 1056: 532,\n",
       " 12341: 533,\n",
       " 20355: 534,\n",
       " 10016: 535,\n",
       " 419: 536,\n",
       " 3215: 537,\n",
       " 2311: 538,\n",
       " 1394: 539,\n",
       " 539: 540,\n",
       " 781: 541,\n",
       " 6709: 542,\n",
       " 3711: 543,\n",
       " 837: 544,\n",
       " 2760: 545,\n",
       " 1232: 546,\n",
       " 7959: 547,\n",
       " 6525: 548,\n",
       " 5139: 549,\n",
       " 2208: 550,\n",
       " 3874: 551,\n",
       " 1539: 552,\n",
       " 1289: 553,\n",
       " 5613: 554,\n",
       " 1114: 555,\n",
       " 49: 556,\n",
       " 390: 557,\n",
       " 8562: 558,\n",
       " 917: 559,\n",
       " 2325: 560,\n",
       " 1341: 561,\n",
       " 2297: 562,\n",
       " 980: 563,\n",
       " 3: 564,\n",
       " 807: 565,\n",
       " 58: 566,\n",
       " 79: 567,\n",
       " 3102: 568,\n",
       " 464: 569,\n",
       " 378: 570,\n",
       " 2524: 571,\n",
       " 404: 572,\n",
       " 2117: 573,\n",
       " 6355: 574,\n",
       " 4961: 575,\n",
       " 50: 576,\n",
       " 1125: 577,\n",
       " 471: 578,\n",
       " 1463: 579,\n",
       " 2247: 580,\n",
       " 737: 581,\n",
       " 867: 582,\n",
       " 2209: 583,\n",
       " 2971: 584,\n",
       " 759: 585,\n",
       " 3226: 586,\n",
       " 3030: 587,\n",
       " 7083: 588,\n",
       " 12055: 589,\n",
       " 5288: 590,\n",
       " 2398: 591,\n",
       " 173: 592,\n",
       " 2593: 593,\n",
       " 2212: 594,\n",
       " 2658: 595,\n",
       " 1374: 596,\n",
       " 3137: 597,\n",
       " 43: 598,\n",
       " 1320: 599,\n",
       " 2588: 600,\n",
       " 1943: 601,\n",
       " 4417: 602,\n",
       " 1677: 603,\n",
       " 12874: 604,\n",
       " 5712: 605,\n",
       " 2279: 606,\n",
       " 4611: 607,\n",
       " 1898: 608,\n",
       " 10442: 609,\n",
       " 18635: 610,\n",
       " 1074: 611,\n",
       " 7846: 612,\n",
       " 679: 613,\n",
       " 1571: 614,\n",
       " 2699: 615,\n",
       " 815: 616,\n",
       " 232: 617,\n",
       " 7768: 618,\n",
       " 2259: 619,\n",
       " 2090: 620,\n",
       " 2192: 621,\n",
       " 2296: 622,\n",
       " 5368: 623,\n",
       " 1950: 624,\n",
       " 935: 625,\n",
       " 6957: 626,\n",
       " 8425: 627,\n",
       " 1392: 628,\n",
       " 2326: 629,\n",
       " 2552: 630,\n",
       " 884: 631,\n",
       " 3099: 632,\n",
       " 939: 633,\n",
       " 5188: 634,\n",
       " 200: 635,\n",
       " 2173: 636,\n",
       " 1070: 637,\n",
       " 1178: 638,\n",
       " 396: 639,\n",
       " 9575: 640,\n",
       " 1736: 641,\n",
       " 368: 642,\n",
       " 1732: 643,\n",
       " 416: 644,\n",
       " 2489: 645,\n",
       " 3518: 646,\n",
       " 325: 647,\n",
       " 1465: 648,\n",
       " 6704: 649,\n",
       " 4275: 650,\n",
       " 270: 651,\n",
       " 7681: 652,\n",
       " 633: 653,\n",
       " 262: 654,\n",
       " 8150: 655,\n",
       " 9211: 656,\n",
       " 730: 657,\n",
       " 5731: 658,\n",
       " 305: 659,\n",
       " 382: 660,\n",
       " 1429: 661,\n",
       " 6566: 662,\n",
       " 119: 663,\n",
       " 483: 664,\n",
       " 1204: 665,\n",
       " 538: 666,\n",
       " 3385: 667,\n",
       " 150: 668,\n",
       " 6703: 669,\n",
       " 1882: 670,\n",
       " 961: 671,\n",
       " 13067: 672,\n",
       " 455: 673,\n",
       " 5798: 674,\n",
       " 1657: 675,\n",
       " 29: 676,\n",
       " 358: 677,\n",
       " 11137: 678,\n",
       " 590: 679,\n",
       " 2187: 680,\n",
       " 2923: 681,\n",
       " 168: 682,\n",
       " 2087: 683,\n",
       " 2272: 684,\n",
       " 14: 685,\n",
       " 469: 686,\n",
       " 1997: 687,\n",
       " 4598: 688,\n",
       " 504: 689,\n",
       " 798: 690,\n",
       " 5302: 691,\n",
       " 3380: 692,\n",
       " 3432: 693,\n",
       " 2217: 694,\n",
       " 2098: 695,\n",
       " 585: 696,\n",
       " 6668: 697,\n",
       " 4292: 698,\n",
       " 650: 699,\n",
       " 6532: 700,\n",
       " 3450: 701,\n",
       " 2472: 702,\n",
       " 4667: 703,\n",
       " 1352: 704,\n",
       " 4311: 705,\n",
       " 1316: 706,\n",
       " 7791: 707,\n",
       " 573: 708,\n",
       " 8706: 709,\n",
       " 6582: 710,\n",
       " 312: 711,\n",
       " 1478: 712,\n",
       " 4559: 713,\n",
       " 1835: 714,\n",
       " 149: 715,\n",
       " 4154: 716,\n",
       " 1383: 717,\n",
       " 9204: 718,\n",
       " 26: 719,\n",
       " 5735: 720,\n",
       " 392: 721,\n",
       " 1231: 722,\n",
       " 4378: 723,\n",
       " 843: 724,\n",
       " 908: 725,\n",
       " 351: 726,\n",
       " 1132: 727,\n",
       " 2375: 728,\n",
       " 1590: 729,\n",
       " 331: 730,\n",
       " 1107: 731,\n",
       " 7177: 732,\n",
       " 1153: 733,\n",
       " 468: 734,\n",
       " 10242: 735,\n",
       " 6769: 736,\n",
       " 375: 737,\n",
       " 453: 738,\n",
       " 224: 739,\n",
       " 5588: 740,\n",
       " 5807: 741,\n",
       " 1550: 742,\n",
       " 1579: 743,\n",
       " 2298: 744,\n",
       " 2455: 745,\n",
       " 13017: 746,\n",
       " 5559: 747,\n",
       " 3157: 748,\n",
       " 1540: 749,\n",
       " 16144: 750,\n",
       " 5414: 751,\n",
       " 2494: 752,\n",
       " 519: 753,\n",
       " 1456: 754,\n",
       " 1298: 755,\n",
       " 2136: 756,\n",
       " 17676: 757,\n",
       " 1587: 758,\n",
       " 6842: 759,\n",
       " 5746: 760,\n",
       " 5699: 761,\n",
       " 18470: 762,\n",
       " 3893: 763,\n",
       " 1872: 764,\n",
       " 410: 765,\n",
       " 12795: 766,\n",
       " 1207: 767,\n",
       " 2770: 768,\n",
       " 201: 769,\n",
       " 249: 770,\n",
       " 1140: 771,\n",
       " 3693: 772,\n",
       " 5650: 773,\n",
       " 605: 774,\n",
       " 5619: 775,\n",
       " 3419: 776,\n",
       " 1747: 777,\n",
       " 2492: 778,\n",
       " 1168: 779,\n",
       " 2831: 780,\n",
       " 20820: 781,\n",
       " 3342: 782,\n",
       " 10665: 783,\n",
       " 2590: 784,\n",
       " 112: 785,\n",
       " 499: 786,\n",
       " 1039: 787,\n",
       " 102: 788,\n",
       " 113: 789,\n",
       " 457: 790,\n",
       " 680: 791,\n",
       " 184: 792,\n",
       " 8996: 793,\n",
       " 493: 794,\n",
       " 1042: 795,\n",
       " 8103: 796,\n",
       " 3093: 797,\n",
       " 7834: 798,\n",
       " 1957: 799,\n",
       " 1965: 800,\n",
       " 1440: 801,\n",
       " 128: 802,\n",
       " 2228: 803,\n",
       " 1816: 804,\n",
       " 1238: 805,\n",
       " 1531: 806,\n",
       " 688: 807,\n",
       " 2570: 808,\n",
       " 1124: 809,\n",
       " 2544: 810,\n",
       " 9480: 811,\n",
       " 4685: 812,\n",
       " 4458: 813,\n",
       " 15951: 814,\n",
       " 6274: 815,\n",
       " 3480: 816,\n",
       " 1922: 817,\n",
       " 648: 818,\n",
       " 11608: 819,\n",
       " 612: 820,\n",
       " 355: 821,\n",
       " 6687: 822,\n",
       " 1521: 823,\n",
       " 3857: 824,\n",
       " 2862: 825,\n",
       " 1032: 826,\n",
       " 436: 827,\n",
       " 3775: 828,\n",
       " 36: 829,\n",
       " 1288: 830,\n",
       " 4953: 831,\n",
       " 6788: 832,\n",
       " 2206: 833,\n",
       " 1449: 834,\n",
       " 5912: 835,\n",
       " 12156: 836,\n",
       " 15669: 837,\n",
       " 599: 838,\n",
       " 7520: 839,\n",
       " 13171: 840,\n",
       " 9097: 841,\n",
       " 2476: 842,\n",
       " 1873: 843,\n",
       " 607: 844,\n",
       " 1424: 845,\n",
       " 2175: 846,\n",
       " 255: 847,\n",
       " 402: 848,\n",
       " 1960: 849,\n",
       " 954: 850,\n",
       " 5901: 851,\n",
       " 1547: 852,\n",
       " 301: 853,\n",
       " 1208: 854,\n",
       " 326: 855,\n",
       " 2994: 856,\n",
       " 3108: 857,\n",
       " 826: 858,\n",
       " 3876: 859,\n",
       " 6278: 860,\n",
       " 787: 861,\n",
       " 1137: 862,\n",
       " 428: 863,\n",
       " 100: 864,\n",
       " 2301: 865,\n",
       " 188: 866,\n",
       " 2267: 867,\n",
       " 4711: 868,\n",
       " 6418: 869,\n",
       " 1027: 870,\n",
       " 13943: 871,\n",
       " 1243: 872,\n",
       " 2562: 873,\n",
       " 3407: 874,\n",
       " 991: 875,\n",
       " 5673: 876,\n",
       " 1644: 877,\n",
       " 8692: 878,\n",
       " 332: 879,\n",
       " 9576: 880,\n",
       " 2976: 881,\n",
       " 225: 882,\n",
       " 4168: 883,\n",
       " 229: 884,\n",
       " 5121: 885,\n",
       " 7333: 886,\n",
       " 6683: 887,\n",
       " 5938: 888,\n",
       " 2806: 889,\n",
       " 1346: 890,\n",
       " 293: 891,\n",
       " 14593: 892,\n",
       " 4873: 893,\n",
       " 3241: 894,\n",
       " 297: 895,\n",
       " 10594: 896,\n",
       " 3598: 897,\n",
       " 2948: 898,\n",
       " 960: 899,\n",
       " 182: 900,\n",
       " 3162: 901,\n",
       " 2804: 902,\n",
       " 558: 903,\n",
       " 664: 904,\n",
       " 8531: 905,\n",
       " 1830: 906,\n",
       " 2041: 907,\n",
       " 9561: 908,\n",
       " 1087: 909,\n",
       " 560: 910,\n",
       " 3771: 911,\n",
       " 1313: 912,\n",
       " 343: 913,\n",
       " 822: 914,\n",
       " 2468: 915,\n",
       " 1726: 916,\n",
       " 9793: 917,\n",
       " 4870: 918,\n",
       " 3490: 919,\n",
       " 5857: 920,\n",
       " 3029: 921,\n",
       " 14620: 922,\n",
       " 969: 923,\n",
       " 1059: 924,\n",
       " 2748: 925,\n",
       " 7654: 926,\n",
       " 1777: 927,\n",
       " 86: 928,\n",
       " 442: 929,\n",
       " 3937: 930,\n",
       " 2214: 931,\n",
       " 3325: 932,\n",
       " 3715: 933,\n",
       " 4271: 934,\n",
       " 2673: 935,\n",
       " 5838: 936,\n",
       " 661: 937,\n",
       " 914: 938,\n",
       " 1574: 939,\n",
       " 5589: 940,\n",
       " 6667: 941,\n",
       " 2683: 942,\n",
       " 7455: 943,\n",
       " 7183: 944,\n",
       " 3199: 945,\n",
       " 12324: 946,\n",
       " 2170: 947,\n",
       " 3333: 948,\n",
       " 2082: 949,\n",
       " 6430: 950,\n",
       " 1586: 951,\n",
       " 1091: 952,\n",
       " 4753: 953,\n",
       " 2122: 954,\n",
       " 18054: 955,\n",
       " 1497: 956,\n",
       " 3451: 957,\n",
       " 6486: 958,\n",
       " 10757: 959,\n",
       " 1224: 960,\n",
       " 6718: 961,\n",
       " 2356: 962,\n",
       " 2025: 963,\n",
       " 269: 964,\n",
       " 281: 965,\n",
       " 7950: 966,\n",
       " 3869: 967,\n",
       " 3803: 968,\n",
       " 2332: 969,\n",
       " 2010: 970,\n",
       " 1520: 971,\n",
       " 2360: 972,\n",
       " 8231: 973,\n",
       " 394: 974,\n",
       " 6937: 975,\n",
       " 211: 976,\n",
       " 8880: 977,\n",
       " 5636: 978,\n",
       " 1363: 979,\n",
       " 770: 980,\n",
       " 891: 981,\n",
       " 7547: 982,\n",
       " 8193: 983,\n",
       " 12267: 984,\n",
       " 10243: 985,\n",
       " 1656: 986,\n",
       " 4993: 987,\n",
       " 500: 988,\n",
       " 3665: 989,\n",
       " 296: 990,\n",
       " 1349: 991,\n",
       " 5832: 992,\n",
       " 95: 993,\n",
       " 1910: 994,\n",
       " 6013: 995,\n",
       " 5928: 996,\n",
       " 2767: 997,\n",
       " 1575: 998,\n",
       " 6406: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_inds_to_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
